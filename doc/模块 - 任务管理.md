# 任务管理

### task.rs

目前控制用户程序的基本块是 `TaskControlBlock`，它比较像线程，是程序调度的基本单位，但不存在一个进程的基本块。这是因为OS实现了更多的 clone 参数，由此使得**每个TCB的地址空间、文件描述符、信号等都是可灵活重用的**。这是 Maturin 和其他OS在处理“任务”概念时的最大区别。

```rust
/// 任务控制块，包含一个用户程序的所有状态信息，但不包括与调度有关的信息。
/// 默认在TCB的外层对其的访问不会冲突，所以外部没有用锁保护，内部的 mutex 仅用来提供可变性
/// 
/// 目前来说，TCB外层可能是调度器或者 CpuLocal：
/// 1. 如果它在调度器里，则 Scheduler 内部不会修改它，且从 Scheduler 里取出或者放入 TCB 是由调度器外部的 Mutex 保护的；
/// 2. 如果它在 CpuLocal 里，则同时只会有一个核可以访问它，也不会冲突。
pub struct TaskControlBlock {
    /// 用户程序的内核栈，内部包含申请的内存空间
    /// 因为 struct 内部保存了页帧 Frame，所以 Drop 这个结构体时也会自动释放这段内存
    pub kernel_stack: KernelStack,
    /// 进程 id。创建任务时实际分配的是 tid 而不是 pid，所以没有对应的 Pid 结构保护
    pub pid: usize,
    /// 线程 id
    pub tid: Tid,
    /// 当退出时是否向父进程发送信号 SIGCHLD。
    /// 如果创建时带 CLONE_THREAD 选项，则不发送信号，除非它是线程组(即拥有相同pid的所有线程)中最后一个退出的线程；
    /// 否则发送信号
    pub send_sigchld_when_exit: bool,
    /// 信号量对应的一组处理函数。
    /// 因为发送信号是通过 pid/tid 查找的，因此放在 inner 中一起调用时更容易导致死锁
    pub signal_handlers: Arc<Mutex<SignalHandlers>>,
    /// 接收信号的结构。
    pub signal_receivers: Arc<Mutex<SignalReceivers>>,
    /// 任务的内存段(内含页表)，同时包括用户态和内核态
    pub vm: Arc<Mutex<MemorySet>>,
    /// 管理进程的所有文件描述符
    pub fd_manager: Arc<Mutex<FdManager>>,
    /// 任务的运行时间信息
    pub time: Mutex<TimeStat>,
    /// 任务的状态信息
    pub inner: Arc<Mutex<TaskControlBlockInner>>,
}
```

这导致很多原本对“进程”的操作根据所属的模块不同，被分成了更具体的语义。例：

- 线程A mmap 了一块空间，不存在一个“进程”使得其中的线程能看见这块空间，而是 clone 时带 CLONE_VM 参数的线程才能看见它。

- 线程A通过sigaction设了一个action，然后通过tkill发信号给B。B能收到对应的信号，但只有它和A的 SignalHandlers 是同一个时才会按 A 的设置处理信号。这当且仅当clone时设置了 CLONE_SIGHAND 参数

- pid对应的“多个线程的集合”也还是存在的，只是判定依据变成了"clone时设置了 CLONE_SIGHAND 参数"。它们之间可以有不同的地址空间、fd等，也可以相同。

- (这一条是设想)线程A通过kill发一个信号，则会首先找到对应tid的TCB，然后往它的 SignalReceivers 里塞一个信号。每个拥有这个 Receivers 的线程都可能发现这个信号，但因为 mutex 它们不会同时发现。每个抢到 mutex 锁的线程会试图比对自己的 mask，如果可以接收则会处理并删除这个信号。这符合 linux 对于"每个线程都有自己的信号掩码，但信号只会发给一个线程，异步信号任取一个没有阻塞的线程"的要求。

musl-libc 的 pthread-create 没有使用上面这些复杂灵活的处理，所以 zCore 那样不管 clone 时的 flags 混过去也是行得通的。

所以目前我们OS的思路只是提供了这样一种支持，对 musl-libc 不是必须的。

另外，分开不同的模块可以让不同的线程同时访问，但也容易死锁。目前的策略是，在处理syscall时，先按照上面TCB里定义的顺序拿所有需要的模块的锁，再进行操作。

### kernel_stack (用户程序的)内核栈

会分配并保存一段连续的页帧，而不是像 rCore 那样直接默认在一个根据 pid 固定偏移的地址上。

在内核态时，这段内存是在 physical memory 上的，因此可以直接访问

这省去了修改 MemorySet 和页表的步骤，比较快，但也意味着没有 shadow page，因为 physical memory 中的所有页都有 READ/WRITE 权限，需要由其他机制实现 shadow page。

### scheduler 调度器

调度器目前使用简单的 FIFO 调度。

### cpu_local

一些需要以 cpu 的视角来处理的工作： run_user 循环、handle_signal、handle_page_fault、**handle_zombie_task**(有一个稍微麻烦一点的去死锁策略)：

处理退出的任务：

将它的子进程全部交给初始进程 ORIGIN_USER_PROC，然后标记当前进程的状态为 Zombie。

这里会需要获取当前核正在运行的用户程序、ORIGIN_USER_PROC、所有子进程的锁。

这里每修改一个子进程的 parent 指针，都要重新用 try_lock 拿子进程的锁和 ORIGIN_USER_PROC 的锁。

如果不用 try_lock ，则可能出现如下的死锁情况：

1.当前进程和子进程都在这个函数里

2.当前进程拿到了 ORIGIN_USER_PROC 的锁，而子进程在函数开头等待 ORIGIN_USER_PROC 的锁

3.当前进程尝试修改子进程的 parent，但无法修改。因为子进程一直拿着自己的锁，它只是在等 ORIGIN_USER_PROC

使用 try_lock 之后，如果出现多个父子进程竞争锁的情况，那么：

1.如果拿到 ORIGIN_USER_PROC 的锁的进程的子进程都没有在竞争这个锁，那么它一定可以顺利拿到自己的所有子进程的锁，并正常执行完成。

2.否则，它会因为无法拿到自己的某个子进程的锁而暂时放弃 ORIGIN_USER_PROC 的锁。

因为进程之间的 parent/children 关系是一棵树，所以在任意时刻一定会有上述第一种情况的进程存在。

所以卡在这个函数上的进程最终一定能以某种顺序依次执行完成，也就消除了死锁。

### time_stat

统计进程的用户态和内核态时间，包含用户态时间统计、内核态时间统计、计时器功能。在计时器触发时，会根据计时器类型给对应线程发送 SIGALRM 或 SIGVTALRM 或 SIGPROF 信号

用于 sys_getrusage  / sys_times / sys_settimer / sys_gettimer 等系统调用

目前这个模块的逻辑如下：

- 在 `cpu_local.rs: run_tasks()` 中切换进入/切出用户程序上下文处，开始/停止统计内核态时间

- 在 `trap/mod.rs: trap_handler()` 中进入/退出异常中断处理时，开始/停止统计内核态时间，停止/开始统计用户态时间

> 如果在 trap 的过程中，通过其他方式退出了进程，那么内核时间统计会在 `run_tasks()` 切出时中断。
> 
> 这样统计的时间仍然是对的
